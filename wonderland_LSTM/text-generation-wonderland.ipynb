{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text-generation-wonderland.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"VqWvDElz9Oxy","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount._DEBUG = True\n","drive.mount('/content/drive', force_remount=True)\n","\n","import os\n","os.chdir('drive/My Drive/Colab Notebooks/text generation/final_project1/wonderland_LSTM')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IgWPoQ_o95jb","colab_type":"code","outputId":"f02d0fb4-dacd-46c5-ba5e-075c5087b931","executionInfo":{"status":"ok","timestamp":1560705250210,"user_tz":-270,"elapsed":2076,"user":{"displayName":"Maryam Meghdadi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDYZFYoOHq4nUb1TsqPOrS_QeWo3iC1Xf8V49ha3w=s64","userId":"09966209550150484662"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.callbacks import ModelCheckpoint\n","from keras.utils import np_utils"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"gRpMqEho-ins","colab_type":"code","colab":{}},"source":["# load ascii text and covert to lowercase\n","filename = \"wonderland.txt\"\n","raw_text = open(filename).read()\n","raw_text = raw_text.lower()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sB3uI9OR-sJ3","colab_type":"code","colab":{}},"source":["# create mapping of unique chars to integers\n","chars = sorted(list(set(raw_text)))\n","char_to_int = dict((c, i) for i, c in enumerate(chars))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zT6EO6M2-349","colab_type":"code","outputId":"7bb7dd91-eb78-4b12-c380-41213422f0e8","executionInfo":{"status":"ok","timestamp":1560705253770,"user_tz":-270,"elapsed":1291,"user":{"displayName":"Maryam Meghdadi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDYZFYoOHq4nUb1TsqPOrS_QeWo3iC1Xf8V49ha3w=s64","userId":"09966209550150484662"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["n_chars = len(raw_text)\n","n_vocab = len(chars)\n","print(\"Total Characters: \", n_chars)\n","print(\"Total Vocab: \",n_vocab)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Total Characters:  144413\n","Total Vocab:  47\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cox3jsSD_JUn","colab_type":"code","outputId":"69dedf6f-ecd5-43ea-d65a-2e53b5748608","executionInfo":{"status":"ok","timestamp":1560705256283,"user_tz":-270,"elapsed":3293,"user":{"displayName":"Maryam Meghdadi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDYZFYoOHq4nUb1TsqPOrS_QeWo3iC1Xf8V49ha3w=s64","userId":"09966209550150484662"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# prepare the dataset of input to output pairs encoded as integers\n","seq_length = 100\n","dataX = []\n","dataY = []\n","for i in range(0, n_chars - seq_length, 1):\n","\tseq_in = raw_text[i:i + seq_length]\n","\tseq_out = raw_text[i + seq_length]\n","\tdataX.append([char_to_int[char] for char in seq_in])\n","\tdataY.append(char_to_int[seq_out])\n","n_patterns = len(dataX)\n","print(\"Total Patterns: \", n_patterns)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Total Patterns:  144313\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S50RoaPKA4AD","colab_type":"code","colab":{}},"source":["# reshape X to be [samples, time steps, features]\n","X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n","# normalize\n","X = X / float(n_vocab)\n","# one hot encode the output variable\n","y = np_utils.to_categorical(dataY)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L-xZ5tHjsa7R","colab_type":"code","outputId":"768f39d2-8648-4f48-ce6c-004d64cf7147","executionInfo":{"status":"ok","timestamp":1560705257081,"user_tz":-270,"elapsed":2720,"user":{"displayName":"Maryam Meghdadi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDYZFYoOHq4nUb1TsqPOrS_QeWo3iC1Xf8V49ha3w=s64","userId":"09966209550150484662"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X.shape"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(144313, 100, 1)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"g45x2-cQB3Ao","colab_type":"code","colab":{}},"source":["# define the LSTM model\n","model = Sequential()\n","model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(256, return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(256))\n","model.add(Dropout(0.2))\n","model.add(Dense(y.shape[1], activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"26yIN7l6fs0w","colab_type":"code","outputId":"390669aa-4581-45f6-b1bf-1e3f598fa269","executionInfo":{"status":"ok","timestamp":1560705258114,"user_tz":-270,"elapsed":1080,"user":{"displayName":"Maryam Meghdadi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDYZFYoOHq4nUb1TsqPOrS_QeWo3iC1Xf8V49ha3w=s64","userId":"09966209550150484662"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["model.summary()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","lstm_1 (LSTM)                (None, 100, 256)          264192    \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 100, 256)          0         \n","_________________________________________________________________\n","lstm_2 (LSTM)                (None, 100, 256)          525312    \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 100, 256)          0         \n","_________________________________________________________________\n","lstm_3 (LSTM)                (None, 256)               525312    \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 47)                12079     \n","=================================================================\n","Total params: 1,326,895\n","Trainable params: 1,326,895\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PdcZ2NyzCRwg","colab_type":"code","colab":{}},"source":["# define the checkpoint\n","filepath=\"weights61+14-improvement1-{epoch:02d}-{loss:.4f}.hdf5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n","callbacks_list = [checkpoint]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JW_OxDfeC6-7","colab_type":"code","colab":{}},"source":["model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XI6-v3eZP2VC","colab_type":"code","colab":{}},"source":["# load the network weights\n","filename = \"weights61+14-improvement1-06-0.9166.hdf5\"\n","model.load_weights(filename)\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KXwbNynNYhED","colab_type":"code","outputId":"1653a57b-75bc-4939-f590-63ea5dc9ebeb","executionInfo":{"status":"ok","timestamp":1560705732436,"user_tz":-270,"elapsed":154743,"user":{"displayName":"Maryam Meghdadi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDYZFYoOHq4nUb1TsqPOrS_QeWo3iC1Xf8V49ha3w=s64","userId":"09966209550150484662"}},"colab":{"base_uri":"https://localhost:8080/","height":799}},"source":["import sys\n","int_to_char = dict((i, c) for i, c in enumerate(chars))\n","# pick a random seed\n","start = numpy.random.randint(0, len(dataX)-1)\n","pattern = dataX[start]\n","print(\"Seed:\")\n","print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n","print()\n","# generate characters\n","for i in range(1000):\n","\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n","\tx = x / float(n_vocab)\n","\tprediction = model.predict(x, verbose=0)\n","\tindex = numpy.argmax(prediction)\n","\tresult = int_to_char[index]\n","\tseq_in = [int_to_char[value] for value in pattern]\n","\tsys.stdout.write(result)\n","\tpattern.append(index)\n","\tpattern = pattern[1:len(pattern)]\n","print(\"\\nDone.\")"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Seed:\n","\" contemptuously.\n","‘i dare say you never even spoke to time!’\n","\n","‘perhaps not,’ alice cautiously replied: \"\n","\n"," ‘bnd that’s the trial of course.’\n","\n","‘it is a little girl,’ said the mock turtle.\n","\n","‘not i!’ said the mock turtle.\n","\n","‘wery much alo to this with,’ said the mock turtle.\n","\n","‘wery much alo to this with,’ said the mock turtle.\n","\n","‘wery much alo to this with,’ said the mock turtle.\n","\n","‘wery much alo to this with,’ said the mock turtle.\n","\n","‘wery much alo to this with,’ said the mock turtle.\n","\n","‘wery much alo to this with,’ said the mock turtle.\n","\n","‘wery much alo to this with,’ said the mock turtle.\n","\n","‘wery much alo to this with,’ said the mock turtle.\n","\n","‘wery much alo to this with,’ said the mock turtle.\n","\n","‘wery much alo to this with,’ said the mock turtle.\n","\n","‘wery much alo to this with,’ said the mock turtle.\n","\n","‘wery much alo to this with,’ said the mock turtle.\n","\n","‘wery much alo to this with,’ said the mock turtle.\n","\n","‘wery much alo to this with,’ said the mock turtle.\n","\n","‘wery much alo to this with,’ said the mock turtle.\n","\n","‘wery much alo to this with,’ said the mock turtle.\n","\n","‘wery much alo to this with,’ said the\n","Done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2fCI2dKgdNiI","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}